# Configure aws cli with our current region as default:
export ACCOUNT_ID=$(aws sts get-caller-identity --output text --query Account)
export AWS_REGION=$(curl -s 169.254.169.254/latest/dynamic/instance-identity/document | jq -r '.region')
echo "export ACCOUNT_ID=${ACCOUNT_ID}" >> ~/.bash_profile
echo "export AWS_REGION=${AWS_REGION}" >> ~/.bash_profile
aws configure set default.region ${AWS_REGION}
aws configure get default.region

#Create an SSH key
#INFORMATION
#Starting from here, when you see command to be entered such as below, you will enter these commands into Cloud9 IDE. You can use the Copy to clipboard feature (right hand upper corner) to simply copy and paste into Cloud9. In order to paste, you can use Ctrl + V for Windows or Command + V for Mac.
#Please run this command to generate SSH Key in Cloud9. This key will be used on the worker node instances to allow ssh access if necessary.
ssh-keygen
#INFORMATION
#Press enter 3 times to take the default choices
#Upload the public key to your EC2 region:
aws ec2 import-key-pair --key-name "mythicaleks" --public-key-material file://~/.ssh/id_rsa.pub
#If you got an error similar to An error occurred (InvalidKey.Format) when calling the ImportKeyPair operation: Key is not in valid OpenSSH public key format then you can try this command instead:
aws ec2 import-key-pair --key-name "mythicaleks" --public-key-material fileb://~/.ssh/id_rsa.pub

#Docker basics
#You just learned how containers are revolutionizing the way software organizations build, test, and deploy software. You've also learned about a container runtime called Docker, which provides an easy interface to package software in the containers and deploy them across one or more different platforms.
#To get you warmed up today, we're going to explore some of the fundamentals of Docker and how to use it locally within one machine. Or in this case, within your AWS Cloud9 workspace.

#Getting to know Docker
docker --version
docker pull nginx\:latest
docker images
#Try docker run -d -p 8080:80 --name nginx nginx\:latest to instantiate the nginx image as a background daemon with port 8080 on the host forwarding through to port 80 within the container
docker run -d -p 8080:80 --name nginx nginx\:latest
#Run docker ps to see that our nginx container is running.
docker ps
#Try curl http://localhost:8080 to use the nginx container and verify it is working with its default index.html.
curl http://localhost:8080
#Running docker logs nginx shows us the logs produced by nginx and the container. You should see some events that correspond to our curl requests.
docker logs nginx
#Use docker exec -it nginx /bin/bash to start an interactive shell into the container's filesystem and constraints
docker exec -it nginx /bin/bash
#To remove the container once and for all, use the docker rm nginx command.
docker rm nginx
#docker rmi nginx\:latest to remove the nginx image from our machine's local cache
docker rmi nginx\:latest

#Building a container image
docker build -t monolith-service .
docker run
TABLE_NAME=$(aws dynamodb list-tables | jq -r .TableNames[0])
docker run -p 8000:80 -e AWS_DEFAULT_REGION=$AWS_REGION -e DDB_TABLE_NAME=$TABLE_NAME monolith-service
docker images
docker logs id

#Tag and push your container image to the monolith repository – ECR of AWS:
MONO_ECR_REPOSITORY_URI=$(aws ecr describe-repositories | jq -r .repositories[].repositoryUri | grep mono)
docker tag monolith-service:latest $MONO_ECR_REPOSITORY_URI:latest
docker push $MONO_ECR_REPOSITORY_URI:latest

EKS
#Document: https://eksctl.io/
#To help facilitate the workshop and interactions with your Amazon EKS clusters, we will install the following binaries:
•	awscliv2  - Used to manage AWS services. This lab will also make use the AWS IAM Authenticator for Kubernetes  in the AWS CLI.
•	eksctl  - Used to create and manage Amazon EKS clusters.
•	kubectl  - Provides a command-line interface for communicating with the Kubernetes control plane.
•	helm  - Kubernetes application management utility.
•	jq  - Command-line JSON processor.
•	yq  - Command-line YAML processor. Note that we will install this as a convenience - it is not specifically required for this workshop.
•	bash-completion  and envsubst  - Utilities to help with shell commands.
•	Information
•	In this workshop we will give you the commands to download the Linux binaries. If you are running Mac OSX / Windows, please see the official EKS docs for the download links. 

#Use the following script to install, configure, and test the required binaries:
cd ~/environment

# install or upgrade the aws cli
sudo pip uninstall -y awscli
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip -o awscliv2.zip
sudo ./aws/install --update
. ~/.bash_profile

# install eksctl
curl --silent --location "https://github.com/weaveworks/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" | tar xz -C /tmp
sudo mv -v /tmp/eksctl /usr/local/bin
eksctl version || ( echo "eksctl not found" && exit 1 )

# install kubectl
sudo curl --silent --location -o /usr/local/bin/kubectl https://s3.us-west-2.amazonaws.com/amazon-eks/1.22.6/2022-03-09/bin/linux/amd64/kubectl
sudo chmod +x /usr/local/bin/kubectl
kubectl version --client=true || ( echo "kubectl not found" && exit 1 )

# install helm
curl https://raw.githubusercontent.com/helm/helm/master/scripts/get-helm-3 | bash

# install additional tools
sudo yum -y install jq gettext bash-completion moreutils

# enable bash completion
kubectl completion bash >>  ~/.bash_completion
eksctl completion bash >> ~/.bash_completion
. ~/.bash_completion

# install yq
echo 'yq() {
 docker run --rm -i -v "${PWD}":/workdir mikefarah/yq yq "$@"
}' | tee -a ~/.bashrc && source ~/.bashrc

# make sure all binaries are in the path
for command in kubectl jq envsubst aws eksctl kubectl helm
  do
    which $command &>/dev/null && echo "$command in path" || ( echo "$command NOT FOUND" && exit 1 )
  done

echo 'Prerequisites installed successfully.'

#In order to work with Amazon EKS in AWS Cloud9 , the instance environment requires an update. AWS Cloud9 normally manages IAM (Identity and Access Management) credentials dynamically. This is not currently compatible with the EKS IAM authentication, so we will disable it and rely on an IAM role created by this workshop instead.

#To ensure temporary credentials are not already in place, disable AWS managed temporary credentials  and remove the existing credentials file by executing the following commands in the Cloud9 terminal:
aws cloud9 update-environment --environment-id $C9_PID --managed-credentials-action DISABLE
rm -vf ${HOME}/.aws/credentials
#Now that temporary credentials have been disabled and removed, the Cloud9 Workspace should authenticate using an IAM Role that was pre-created in this environment. To validate this, execute the following command:
aws sts get-caller-identity --profile xxxx

#Create an EKS cluster

1.	Ensure you have completed the prerequisite steps up to this point - all steps under Workshop Setup, Prerequisites, and Update IAM settings for your Workspace.
2.	Create an eksctl deployment file (mythicaleks.yaml) to use in creating your cluster with the following syntax:
cd ~/environment

cat << EOF > xxxxx.yaml
---
apiVersion: eksctl.io/v1alpha5
kind: ClusterConfig

metadata:
  name: xxxxx-eksctl
  region: ${AWS_REGION}
  version: "1.22"

availabilityZones: ["${AWS_REGION}a", "${AWS_REGION}b", "${AWS_REGION}c"]

managedNodeGroups:
- name: nodegroup
  desiredCapacity: 3
  ssh:
    allow: true
    publicKeyName: <publickey>

# To enable all of the control plane logs, uncomment below:
# cloudWatch:
#  clusterLogging:
#    enableTypes: ["*"]

EOF
3.	Use the file created in the previous step as the input for the eksctl cluster creation:
eksctl create cluster -f mythicaleks.yaml
5.4 Test the cluster
Once the cluster has been created, confirm your nodes are in the Ready state with the command:
kubectl get nodes
You should see 3 nodes with Status=Ready, meaning we have correctly authenticated to the cluster, and the 3 nodes have joined successfully the cluster.

5.5 Troubleshooting eksctl Issues (Optional)
#Note
#The information below details the symptoms and solutions to common eksctl failures. Please proceed to the next section if eksctl and cluster testing were successful.

#Issues running eksctl

#SSH Key Error

#If you receive an error regarding non-existent SSH key, please create the SSH key and run the cluster creation command again.

#General API Error

#If your cluster fails to create, try running the command a 2nd time. If the issue persists, work with your lab moderator to identify the root cause.

#Issues testing the EKS cluster

#Credentials API Error

#If you recieve this error when trying to run any kubectl commands for this workshop:
#Unable to connect to the server: getting credentials: exec plugin is configured to use API version client.authentication.k8s.io/v1alpha1, plugin returned version client.authentication.k8s.io/v1beta1Launching
#Execute the following command:
aws eks update-kubeconfig --name <name_cluster>
#After the command completes successfully, retry your kubectl command.
6.1 Provision AWS Load Balancer Controller
#In this lab, we are going to deploy the AWS Load Balancer Controller  - which will be used to expose our applications externally.
#Use the following process to deploy the AWS Load Balancer Controller :
1.	Create an IAM OIDC provider and associate it with your cluster:
eksctl utils associate-iam-oidc-provider --cluster=mythicaleks-eksctl --approve
2.	Download the IAM policy for the AWS Load Balancer Controller
curl -o iam-policy.json https://raw.githubusercontent.com/kubernetes-sigs/aws-load-balancer-controller/main/docs/install/iam_policy.json
3.	Create an IAM policy named AWSLoadBalancerControllerIAMPolicy to allow the AWS Load Balancer controller to make AWS API calls on your behalf and save the Policy.Arn into a new variable called PolicyARN:
#create the policy
aws iam create-policy --policy-name AWSLoadBalancerControllerIAMPolicy --policy-document file://iam-policy.json

#get the policy ARN
export PolicyARN=$(aws iam list-policies --query 'Policies[?PolicyName==`AWSLoadBalancerControllerIAMPolicy`].Arn' --output text)
echo $PolicyARN
#Tip
#It is a best practice to use IAM permissions directly on Pods instead of EC2 Instances. Learn more about IAM Roles for Service Accounts  in the Amazon EKS documentation.
4.	Create a Kubernetes service account and an IAM role (for the pod running the AWS Load Balancer Controller):
eksctl create iamserviceaccount \
   --cluster=mythicaleks-eksctl \
   --namespace=kube-system \
   --name=aws-load-balancer-controller \
   --attach-policy-arn=$PolicyARN \
   --override-existing-serviceaccounts \
   --approve
# is this command doing?
#The command above uses eksctl to create a service account name aws-load-balancer-controller in the namespace kube-system. It will also create an IAM Role using the provided policy, and only allow the previously created service account to assume this role using the OpenID Connect endpoint of the EKS cluster.
5.	Deploy AWS Load Balancer Controller using its Helm Chart:
a. Add the EKS repository to Helm:
helm repo add eks https://aws.github.io/eks-charts
b. Install the TargetGroupBinding CRDs:
kubectl apply -k "github.com/aws/#-charts/stable/aws-load-balancer-controller//crds?ref=master"
c. Install the AWS Load Balancer controller using the previously created service account:
helm upgrade -i aws-load-balancer-controller eks/aws-load-balancer-controller \
  -n kube-system \
  --set clusterName=mythicaleks-eksctl \
  --set serviceAccount.create=false \
  --set serviceAccount.name=aws-load-balancer-controller
#Verify that the deployment was successful and the controller started:
kubectl logs -n kube-system deployments/aws-load-balancer-controller
kubectl -n kube-system get deployments
#Tip
#Look here to find more information on configuring the AWS Load Balancer Controller: Helm configuration parameters .
6.2 Deploy the Official Kubernetes Dashboard
#To gain visibility into how the Mythical Mysfits applications are running within your Amazon EKS cluster, we are going to deploy the official Kubernetes dashboard. The Kubernetes dashboard is not deployed by default, but there are instructions in the official documentation .

#Deploying the dashboard

1.	We can deploy the dashboard with the following command:
export DASHBOARD_VERSION="v2.6.1"
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/${DASHBOARD_VERSION}/aio/deploy/recommended.yaml
2.	Since this is deployed to our private cluster, we need to access it via a proxy. kube-proxy is available to proxy our requests to the dashboard service. In your Cloud9 workspace, run the following command:
kubectl proxy --port=8080 --address=0.0.0.0 --disable-filter=true &
3.	This will start the proxy, listen on port 8080, listen on all interfaces, and will disable the filtering of non-localhost requests. Note that the command will continue to run in the background of the current terminal session.
STOP!
#We are disabling request filtering, a security feature that guards against XSRF attacks. This isn't recommended for a production environment, but is useful for our dev environment.

##Accessing the Kubernetes dashboard

1.	In your Cloud9 workspace, click Tools / Preview / Preview Running Application
2.	Append the following text to the end of the URL:
/api/v1/namespaces/kubernetes-dashboard/services/https:kubernetes-dashboard:/proxy/
3.	The Cloud9 preview browser doesn't support token authentication, so once you have the login screen in the Cloud9 preview browser tab, press the Pop Out button to open the login screen in a regular browser tab:  
4.	Open a new terminal tab (e.g. 'Alt + t', 'Window --> New Terminal', or the green '+' next to an existing terminal tab) and enter the following command:
aws eks get-token --cluster-name mythicaleks-eksctl | jq -r '.status.token'
5.	Copy the output of the previous command and then click the radio button next to Token in the browser. Paste the output in the "Enter token" textbox.  
6.	Click press Sign In.
7.	At this point the Amazon EKS cluster is new and has very few resources deployed. As you explore the dashboard, many of the sections may not have much information. This will change as we begin to deploy our Mythical Mysfits application.

#Checkpoint 

#Nice work! You have prepared the environment for the Mythical Mysfits application and deployed the Kubernetes Dashboard.
#In this lab, we are going to deploy the containerized version of the monolith application as a Kubernetes pod running in your Amazon EKS cluster.
#Tip
#Hopefully you created the monolith container image and pushed it to Amazon ECR earlier in Lab 1. If you didn't, now would be a good time to jump back take care of that.

#Creating an IAM role to enable pods to access the DynamoDB tables

#Our containerized applications will need to access the DynamoDB table that's storing all of the information about our lovely mysfits. We will be making use of the IAM roles for service accounts (IRSA)  feature to grant our pods these permissions.
1.	Switch to the tab where you have your AWS Cloud9 workspace opened.
2.	In Cloud9 cd into the modu
cd /home/ec2-user/environment/amazon-ecs-mythicalmysfits-workshop/workshop-1/app/monolith-service
4.	We now need to create our Kubernetes service account to be using an IAM role with the appropriate rights to access our DynamoDB table:
a. Configure the DynamoDB IAM policy:
TABLE_NAME=$(jq < ../../cfn-output.json -r '.DynamoTable')
cat << EOF > iam-dynamodb-policy.json
{
	"Version": "2012-10-17",
	"Statement": [
		{
			"Effect": "Allow",
			"Action": [
				"dynamodb:*"
			],
			"Resource": "arn:aws:dynamodb:${AWS_REGION}:${ACCOUNT_ID}:table/${TABLE_NAME}"
		}
	]
}
EOF
b. Create the DynamoDB IAM policy:
#create the policy
aws iam create-policy \
				--policy-name MythicalMisfitDynamoDBTablePolicy \
				--policy-document file://iam-dynamodb-policy.json
#get the policy ARN
export PolicyARNDynamoDB=$(aws iam list-policies --query 'Policies[?PolicyName==`MythicalMisfitDynamoDBTablePolicy`].Arn' --output text)
echo $PolicyARNDynamoDB
c. Create the Kubernetes Service Account:
	eksctl create iamserviceaccount \
		--cluster=mythicaleks-eksctl \
		--namespace=default \
		--name=mythical-misfit \
		--attach-policy-arn=$PolicyARNDynamoDB \
		--override-existing-serviceaccounts \
		--approve

  
